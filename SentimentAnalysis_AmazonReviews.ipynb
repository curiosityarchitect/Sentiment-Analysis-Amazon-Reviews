{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "import seaborn as sns\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "#@author: alexwu.tech@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies (may have more)\n",
    "#! pip install contractions\n",
    "# Dataset: https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>52782374</td>\n",
       "      <td>R1PR37BR7G3M6A</td>\n",
       "      <td>B00D7H8XB6</td>\n",
       "      <td>868449945</td>\n",
       "      <td>AmazonBasics 12-Sheet High-Security Micro-Cut ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>24045652</td>\n",
       "      <td>R3BDDDZMZBZDPU</td>\n",
       "      <td>B001XCWP34</td>\n",
       "      <td>33521401</td>\n",
       "      <td>Derwent Colored Pencils, Inktense Ink Pencils,...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace customer_id       review_id  product_id product_parent  \\\n",
       "0          US    43081963  R18RVCKGH1SSI9  B001BM2MAC      307809868   \n",
       "1          US    10951564  R3L4L6LW1PUOFY  B00DZYEXPQ       75004341   \n",
       "2          US    21143145  R2J8AWXWTDX2TF  B00RTMUHDW      529689027   \n",
       "3          US    52782374  R1PR37BR7G3M6A  B00D7H8XB6      868449945   \n",
       "4          US    24045652  R3BDDDZMZBZDPU  B001XCWP34       33521401   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "3  AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
       "4  Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
       "\n",
       "  star_rating helpful_votes total_votes vine verified_purchase  \\\n",
       "0           5             0           0    N                 Y   \n",
       "1           5             0           1    N                 Y   \n",
       "2           5             0           0    N                 Y   \n",
       "3           1             2           3    N                 Y   \n",
       "4           4             0           0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                         Five Stars   \n",
       "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                      but I am sure I will like it.   \n",
       "3  and the shredder was dirty and the bin was par...   \n",
       "4                                         Four Stars   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                                     Great product.  2015-08-31  \n",
       "1  What's to say about this commodity item except...  2015-08-31  \n",
       "2    Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
       "3  Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
       "4                    Gorgeous colors and easy to use  2015-08-31  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('amazon_reviews_us_Office_Products_v1_00.tsv',sep='\\t',on_bad_lines='skip',dtype=str)  \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['star_rating','review_body']].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form two classes and select 50000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316.30319"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#form two classes, ratings 1-3 form class 1, and ratings 4 and 5 form class 2 as a new column 'label'\n",
    "\n",
    "\n",
    "data.dropna(how='any',inplace=True) #drop rows with missing values\n",
    "data['label'] = np.where(pd.to_numeric(data['star_rating']) < 4, 1, 2)\n",
    "#data['label'] = data['star_rating'].apply(lambda x: 1 if pd.to_numeric(x) < 4 else 2)\n",
    "\n",
    "#via. timeit\n",
    "#using apply.(lambda func.):\n",
    "# 4.43 s ± 17.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) \n",
    "#using np.where():\n",
    "# 533 ms ± 2.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) wayyy faster\n",
    "\n",
    "#randomizing to select 50000 samples for each label \n",
    "sampled_data = data.groupby('label').apply(lambda x: x.sample(n=50000,random_state=1)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#obtain average length of reviews (before cleaned and pre-proccessed data)\n",
    "sampled_data['review_body'].str.len().mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304.1103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fix contractions first\n",
    "sampled_data['review_body'] = sampled_data['review_body'].apply(contractions.fix)\n",
    "\n",
    "#remove punctuations, numbers, and special characters\n",
    "sampled_data['review_body'] = sampled_data['review_body'].str.lower().str.replace(r'(http[s]?://\\S+)|(www\\.\\S+)|(<[^>]+>)|[^a-zA-Z]+',' ',regex =True).str.replace(r'\\s+',' ',regex = True) #remove url and html\n",
    "\n",
    "sampled_data['review_body'].str.len().mean() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190.13981"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "sampled_data['review_body'] = sampled_data['review_body'].apply(lambda x: ' '.join([word for word in nltk.word_tokenize(x) if word not in stop_words]))\n",
    "\n",
    "sampled_data['review_body'].str.len().mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('omw-1.4') Open Multilingual Wordnet dependency (extension of nltk.corpus.wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186.94236"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "l = WordNetLemmatizer()\n",
    "sampled_data['review_body'] = sampled_data['review_body'].apply(lambda x: ' '.join([l.lemmatize(word) for word in nltk.word_tokenize(x)]))\n",
    "\n",
    "sampled_data['review_body'].str.len().mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF and BoW Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = sampled_data['label']\n",
    "#BoW --------------------\n",
    "vectorizer = CountVectorizer()\n",
    "X_BoW = vectorizer.fit_transform(sampled_data['review_body'])\n",
    "#X.shape = (100000, 40261) due to more sophisticated vectorization processes, single-character words may have been ommitted \n",
    "\n",
    "#manual method done out of curiosity (less-optimal)\n",
    "'''\n",
    "vocab = set(word for review in sampled_data['review_body'] for word in review.split()) #build a vocab \n",
    "vocab = {word: i for i, word in enumerate(vocab)} #assign index to each word in vocab \n",
    "X_BoW = np.zeros((len(sampled_data['review_body']),len(vocab))) #initialize matrix, #rows as number reviews, #columns as each word in vocab\n",
    "for i, reviews in enumerate(sampled_data['review_body']):\n",
    "    review = reviews.split()\n",
    "    for word in review:\n",
    "        if word in vocab:\n",
    "            X_BoW[i,vocab[word]] += 1\n",
    "#X_BoW.shape = (100000, 40285)\n",
    "'''\n",
    "X_train_BoW,X_test_BoW,Y_train,Y_test = train_test_split(X_BoW,Y,train_size=0.8,random_state=42)\n",
    "#-------------------------\n",
    "\n",
    "#TF-IDF ------------------\n",
    "X_TFIDF = TfidfTransformer().fit_transform(X_BoW) #turn previous vectorized BoW count matrix to TFIDF matrix\n",
    "X_train_TFIDF,X_test_TFIDF,Y_train,Y_test = train_test_split(X_TFIDF,Y,train_size=0.8,random_state=42) \n",
    "#-------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for models below\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn import svm, naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Accuracy:  0.78695\n",
      "TFIDF Accuracy:  0.78505\n",
      "-----------------------\n",
      "BoW Precision:  0.7912043574742788\n",
      "TFIDF Precision:  0.8183129855715872\n",
      "-----------------------\n",
      "BoW Recall:  0.7816641753861485\n",
      "TFIDF Recall:  0.7347284504235176\n",
      "-----------------------\n",
      "BoW F1:  0.7864053336006817\n",
      "TFIDF F1:  0.7742714623260698\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "perceptron_BoW = Perceptron()\n",
    "perceptron_TFIDF = Perceptron()\n",
    "\n",
    "perceptron_BoW.fit(X_train_BoW,Y_train)\n",
    "perceptron_BoW_pred = perceptron_BoW.predict(X_test_BoW)\n",
    "\n",
    "perceptron_TFIDF.fit(X_train_TFIDF,Y_train)\n",
    "percentron_TFIDF_pred = perceptron_TFIDF.predict(X_test_TFIDF)\n",
    "\n",
    "#accuracy \n",
    "print('BoW Accuracy: ',accuracy_score(Y_test,perceptron_BoW_pred))\n",
    "print('TFIDF Accuracy: ',accuracy_score(Y_test,percentron_TFIDF_pred))\n",
    "print('-----------------------')\n",
    "\n",
    "#precision\n",
    "print('BoW Precision: ',precision_score(Y_test,perceptron_BoW_pred))\n",
    "print('TFIDF Precision: ',precision_score(Y_test,percentron_TFIDF_pred))\n",
    "print('-----------------------')\n",
    "\n",
    "#recall\n",
    "print('BoW Recall: ',recall_score(Y_test,perceptron_BoW_pred))\n",
    "print('TFIDF Recall: ',recall_score(Y_test,percentron_TFIDF_pred))\n",
    "print('-----------------------')\n",
    "\n",
    "#f1\n",
    "print('BoW F1: ',f1_score(Y_test,perceptron_BoW_pred))\n",
    "print('TFIDF F1: ',f1_score(Y_test,percentron_TFIDF_pred))\n",
    "print('-----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Accuracy:  0.82405\n",
      "TFIDF Accuracy:  0.83915\n",
      "-----------------------\n",
      "BoW Precision:  0.8386694386694387\n",
      "TFIDF Precision:  0.837190900098912\n",
      "-----------------------\n",
      "BoW Recall:  0.8039860488290982\n",
      "TFIDF Recall:  0.8434479322371699\n",
      "-----------------------\n",
      "BoW F1:  0.8209615873823455\n",
      "TFIDF F1:  0.8403077686770909\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "#note, optimized for linear decision bounds. Will follow up by testing on further unseen data.\n",
    "#if data seems to poorly generalize, will try to use a non-linear kernel follwing gridsearchCV\n",
    "svm_BoW = svm.LinearSVC(max_iter=100000)\n",
    "svm_TFIDF = svm.LinearSVC(max_iter=100000)\n",
    "\n",
    "svm_BoW.fit(X_train_BoW,Y_train)\n",
    "svm_BoW_pred = svm_BoW.predict(X_test_BoW)\n",
    "\n",
    "svm_TFIDF.fit(X_train_TFIDF,Y_train)\n",
    "svm_TFIDF_pred = svm_TFIDF.predict(X_test_TFIDF)\n",
    "\n",
    "#accuracy\n",
    "print('BoW Accuracy: ',accuracy_score(Y_test,svm_BoW_pred))\n",
    "print('TFIDF Accuracy: ',accuracy_score(Y_test,svm_TFIDF_pred))\n",
    "print('-----------------------')\n",
    "\n",
    "#precision\n",
    "print('BoW Precision: ',precision_score(Y_test,svm_BoW_pred))\n",
    "print('TFIDF Precision: ',precision_score(Y_test,svm_TFIDF_pred))\n",
    "print('-----------------------')\n",
    "\n",
    "#recall\n",
    "print('BoW Recall: ',recall_score(Y_test,svm_BoW_pred))\n",
    "print('TFIDF Recall: ',recall_score(Y_test,svm_TFIDF_pred))\n",
    "print('-----------------------')\n",
    "\n",
    "#f1\n",
    "print('BoW F1: ',f1_score(Y_test,svm_BoW_pred))\n",
    "print('TFIDF F1: ',f1_score(Y_test,svm_TFIDF_pred))\n",
    "print('-----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Accuracy:  0.8385\n",
      "TFIDF Accuracy:  0.8488\n",
      "BoW Precision:  0.8520434557682359\n",
      "TFIDF Precision:  0.8441826215022091\n",
      "BoW Recall:  0.820627802690583\n",
      "TFIDF Recall:  0.8568011958146488\n",
      "BoW F1:  0.8360406091370558\n",
      "TFIDF F1:  0.8504451038575668\n"
     ]
    }
   ],
   "source": [
    "logistic_BoW = LogisticRegression(max_iter=100000)\n",
    "logistic_TFIDF = LogisticRegression(max_iter=100000)\n",
    "\n",
    "logistic_BoW.fit(X_train_BoW,Y_train)\n",
    "logistic_BoW_pred = logistic_BoW.predict(X_test_BoW)\n",
    "\n",
    "logistic_TFIDF.fit(X_train_TFIDF,Y_train)\n",
    "logistic_TFIDF_pred = logistic_TFIDF.predict(X_test_TFIDF)\n",
    "\n",
    "#accuracy\n",
    "print('BoW Accuracy: ',accuracy_score(Y_test,logistic_BoW_pred))\n",
    "print('TFIDF Accuracy: ',accuracy_score(Y_test,logistic_TFIDF_pred))\n",
    "\n",
    "#precision\n",
    "print('BoW Precision: ',precision_score(Y_test,logistic_BoW_pred))\n",
    "print('TFIDF Precision: ',precision_score(Y_test,logistic_TFIDF_pred))\n",
    "\n",
    "#recall\n",
    "print('BoW Recall: ',recall_score(Y_test,logistic_BoW_pred))\n",
    "print('TFIDF Recall: ',recall_score(Y_test,logistic_TFIDF_pred))\n",
    "\n",
    "#f1\n",
    "print('BoW F1: ',f1_score(Y_test,logistic_BoW_pred))\n",
    "print('TFIDF F1: ',f1_score(Y_test,logistic_TFIDF_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Accuracy:  0.80365\n",
      "TFIDF Accuracy:  0.81915\n",
      "BoW Precision:  0.8436093609360936\n",
      "TFIDF Precision:  0.8131953933242241\n",
      "BoW Recall:  0.7471848530144495\n",
      "TFIDF Recall:  0.830293971101146\n",
      "BoW F1:  0.7924747661575859\n",
      "TFIDF F1:  0.8216557368966027\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_BoW = naive_bayes.MultinomialNB()\n",
    "naive_bayes_TFIDF = naive_bayes.MultinomialNB()\n",
    "\n",
    "naive_bayes_BoW.fit(X_train_BoW,Y_train)\n",
    "naive_bayes_BoW_pred = naive_bayes_BoW.predict(X_test_BoW)\n",
    "\n",
    "naive_bayes_TFIDF.fit(X_train_TFIDF,Y_train)\n",
    "naive_bayes_TFIDF_pred = naive_bayes_TFIDF.predict(X_test_TFIDF)\n",
    "\n",
    "#accuracy\n",
    "print('BoW Accuracy: ',accuracy_score(Y_test,naive_bayes_BoW_pred))\n",
    "print('TFIDF Accuracy: ',accuracy_score(Y_test,naive_bayes_TFIDF_pred))\n",
    "\n",
    "#precision\n",
    "print('BoW Precision: ',precision_score(Y_test,naive_bayes_BoW_pred))\n",
    "print('TFIDF Precision: ',precision_score(Y_test,naive_bayes_TFIDF_pred))\n",
    "\n",
    "#recall\n",
    "print('BoW Recall: ',recall_score(Y_test,naive_bayes_BoW_pred))\n",
    "print('TFIDF Recall: ',recall_score(Y_test,naive_bayes_TFIDF_pred))\n",
    "\n",
    "#f1\n",
    "print('BoW F1: ',f1_score(Y_test,naive_bayes_BoW_pred))\n",
    "print('TFIDF F1: ',f1_score(Y_test,naive_bayes_TFIDF_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
